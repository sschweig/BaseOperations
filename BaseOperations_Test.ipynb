{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3242965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://data.gdeltproject.org/gdeltv2/20231110150000.export.CSV.zip', 'http://data.gdeltproject.org/gdeltv2/20231110150000.mentions.CSV.zip', 'http://data.gdeltproject.org/gdeltv2/20231110150000.gkg.csv.zip']\n",
      "http://data.gdeltproject.org/gdeltv2/20231110150000.export.CSV.zip\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Too many columns specified: expected 11 and found 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 517\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    516\u001b[0m rh \u001b[38;5;241m=\u001b[39m RunHandler()\n\u001b[1;32m--> 517\u001b[0m \u001b[43mrh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 505\u001b[0m, in \u001b[0;36mRunHandler.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    502\u001b[0m results \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreqs\u001b[38;5;241m.\u001b[39mget_content(url))\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m#Unpack zip\u001b[39;00m\n\u001b[1;32m--> 505\u001b[0m df_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df_content\u001b[38;5;241m.\u001b[39mindex))\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m#Prep\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 360\u001b[0m, in \u001b[0;36mDataHandler._extract\u001b[1;34m(self, zipped)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extract\u001b[39m(\u001b[38;5;28mself\u001b[39m, zipped: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load the content of the zipped export csv into a Pandas DataFrame.\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    We use the columns listed in the spec sheet, ignoring the rest of the \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    358\u001b[0m \n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 360\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m             \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m26\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m52\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m53\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m56\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m57\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m59\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m             \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGLOBALEVENTID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSQLDATE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEventCode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEventBaseCode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEventRootCode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mActionGeo_FullName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mActionGeo_CountryCode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mActionGeo_Lat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mActionGeo_Long\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDATEADDED\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSOURCEURL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m    374\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m             \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEventCode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_conv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEventBaseCode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eventbase_conv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEventRootCode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eventroot_conv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mActionGeo_CountryCode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fips_iso2_conv\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m             \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin-1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m    381\u001b[0m \u001b[43m             \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;124;03m    df = pd.read_csv(zipped, encoding='latin-1')\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m    print(df.head())\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Base_Test\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Base_Test\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Base_Test\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Base_Test\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:920\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:982\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Too many columns specified: expected 11 and found 7"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "import io\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "from sqlalchemy import Column, Integer, Text, Date, Float\n",
    "from geoalchemy2 import Geometry\n",
    "from mappings import EVENT_CODES, EVENT_BASE_CODES, EVENT_ROOT_CODES, MAP_FIPS_TO_ISO2\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class RequestHandler:\n",
    "    \"\"\"\n",
    "    Reusable and extensible request handler. \n",
    "    \n",
    "    Todo\n",
    "    ----\n",
    "    Provide more generalized functionality. I.E. the ability to retrieve the\n",
    "    response object directly, as well as the ability to POST messages.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 total: int = 3, \n",
    "                 backoff: float = 0.1, \n",
    "                 redirect: int= 3) -> None:\n",
    "        \"\"\"RequestHandler init function.\n",
    "        \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        total : int, optional\n",
    "            Total number of times to retry the endpoint when `get_content` is \n",
    "            invoked. The default is 3.\n",
    "        backoff : float, optional\n",
    "            Delay added between consecutive requests in the event an endpoint \n",
    "            fails to respond with a status 200 message. The default is 0.1.\n",
    "        redirect : int, optional\n",
    "            Maximum number of redirects permitted before requests library fails. \n",
    "            The default is 3.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info('Init request handler')\n",
    "        self.sess = requests.Session()\n",
    "        \n",
    "        retries = Retry(\n",
    "            total=total,\n",
    "            backoff_factor=backoff,\n",
    "            redirect=redirect\n",
    "        )\n",
    "        \n",
    "        self.sess.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        logger.info('Request handler initialized')\n",
    "        \n",
    "    def get_content(self, url: str) -> requests.Response:\n",
    "        \"\"\"Fetch response and return the content of the response object.\n",
    "        \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        url : str\n",
    "            Path to target endpoint.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        TYPE\n",
    "            DESCRIPTION.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        resp = self.sess.get(url)\n",
    "        \n",
    "        if resp.status_code==200:\n",
    "            return resp.content\n",
    "        else:\n",
    "            logger.info('Endpoint: {} returned status code: {}'.format(\n",
    "                url, resp.status_code))      \n",
    "        \n",
    "TABLE_NAME = \"threat_table\"\n",
    "Base = declarative_base()\n",
    "\n",
    "class Threat(Base):\n",
    "    \"\"\"SQLalchemy base class for storing and retreiving objects.\n",
    "    \n",
    "    Todo\n",
    "    ----\n",
    "    Decide between Integer and Text for Event Codes.\n",
    "    OPTIMIZATION Consider indexing for increased read performance.\n",
    "    OPTIMIZATION String(n) not text to min. space requirements where possible\n",
    "    \"\"\"\n",
    "    __tablename__ = TABLE_NAME\n",
    "    GLOBALEVENTID = Column(Text, primary_key=True) #A\n",
    "    SQLDATE = Column(Text, nullable=False) #B\n",
    "    EventCode = Column(Text, nullable=False) #Z\n",
    "    EventBaseCode = Column(Text, nullable=False) #AA\n",
    "    EventRootCode = Column(Text, nullable=False) #AB\n",
    "    ActionGeo_FullName = Column(Text, nullable=False) #BA\n",
    "    ActionGeo_CountryCode = Column(Text, nullable=False) #BB\n",
    "    ActionGeo_Lat = Column(Text, nullable=False) #BE\n",
    "    ActionGeo_Long = Column(Text, nullable=False) #BF\n",
    "    ActionGeo_Coords = Column(Geometry('POINT'), nullable=False) # BE + BF\n",
    "    DATEADDED = Column(Text, nullable=False) #BH\n",
    "    SOURCEURL = Column(Text, nullable=False) #BI\n",
    "    \n",
    "class DatabaseHandler:\n",
    "    \"\"\"\n",
    "    Reusable and extensible handler for database connections. \n",
    "    \n",
    "    Todo\n",
    "    ----\n",
    "    Extend fetch functionality to enable queries.\n",
    "    Index database for faster searches\n",
    "    OPTIMIZATION Implement connection pooling functionality for faster concurrent \n",
    "    read/writes.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"DatabaseHandler init function.\n",
    "        \n",
    "        Instatiate connection to Postgres + PostGIS database.\n",
    "        \n",
    "        \"\"\"\n",
    "        logger.info('Init request handler')\n",
    "        db_uri = \"postgresql://{}:{}@{}:5432/{}\".format(\n",
    "            os.environ.get('DB_USER', 'default_user'), \n",
    "            os.environ.get('DB_PW', 'default_pass'), \n",
    "            os.environ.get('DB_HOST', 'localhost'), \n",
    "            os.environ.get('DB_NAME', 'test_db')\n",
    "        )\n",
    "        \n",
    "        self.engine = create_engine(\n",
    "            db_uri\n",
    "        )\n",
    "        \n",
    "        self.conn = self.engine.connect() \n",
    "        Base.metadata.create_all(self.engine)\n",
    "        Session = sessionmaker(bind=self.engine)\n",
    "        self.sess = Session()\n",
    "        \n",
    "        logger.info('Database handler initialized')\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Retrieve contents of database, count the results, and print a few rows.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        res = self.fetch_all()\n",
    "        print('Num rows in db: {}\\n'.format(len(res)))\n",
    "        #logger.info('Num rows in db: {}\\n'.format(len(res)))\n",
    "        \n",
    "        for idx, row in enumerate(res):\n",
    "            if idx>= 50:\n",
    "                break\n",
    "            \n",
    "            print(vars(row))\n",
    "            #logger.info(row)\n",
    "            \n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close the database connection.\n",
    "        \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.sess.close()\n",
    "        self.conn.close()\n",
    "        self.engine.dispose()\n",
    "        logger.info('Database connection safely closed.')\n",
    "\n",
    "    def fetch_all(self) -> list:\n",
    "        \"\"\"Return results matching all rows.\n",
    "        \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            list of rows retrieved from database.\n",
    "            \n",
    "        Todo\n",
    "        ----\n",
    "        Enable querying for particular data. This is sufficient for demo purposes.\n",
    "        OPTIMIZATION Enable indexing\n",
    "\n",
    "        \"\"\"\n",
    "        test_data = self.sess.query(Threat).all()\n",
    "        return test_data\n",
    "    \n",
    "    def upload(self, gdf: gpd.GeoDataFrame()) -> None:\n",
    "        \"\"\"Populate table with the contents of `gdf`.\n",
    "        \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        gdf : gpd.GeoDataFrame()\n",
    "            Geodataframe containing points of interest within the United States\n",
    "            as well as intel sources, dates, and event codings.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \n",
    "        Todo\n",
    "        ----\n",
    "        OPTIMIZATION Enable caching\n",
    "\n",
    "        \"\"\"\n",
    "        gdf.to_postgis(\n",
    "            TABLE_NAME, \n",
    "            self.engine, \n",
    "            if_exists='append', \n",
    "            index=False, \n",
    "            dtype={'ActionGeo_Coords': Geometry('POINT', srid=3857)}\n",
    "        )  \n",
    "        \n",
    "        print('Adding {} rows'.format(len(gdf.index)))\n",
    "        logger.info('GeoDataFrame successfully added to table.')\n",
    "        \n",
    "class DataHandler:\n",
    "    \"\"\"\n",
    "    Custom handler for acquiring, prepping, and storing data from `tar_url` and\n",
    "    associated export files.\n",
    "    \n",
    "    Todo\n",
    "    ----\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    tar_url = 'http://data.gdeltproject.org/gdeltv2/lastupdate.txt'\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"DataHandler init function.\n",
    "        \n",
    "        Instatiate DBHandler and RequestHandler classes.\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info('Init data handler')\n",
    "        self.db = DatabaseHandler()\n",
    "        self.reqs = RequestHandler()\n",
    "        logger.info('Data handler initialized')\n",
    "        \n",
    "    def run(self) -> None:\n",
    "        pass\n",
    "        \n",
    "    def _data_prep(self, df: pd.DataFrame) -> gpd.GeoDataFrame:\n",
    "        \"\"\"Perform transforms on received data to prepare it for storage.\n",
    "        \n",
    "        1. Creates a GeoDataFrame out of the data received from the target \n",
    "            endpoint. Uses Long and Lat to generate a point object containing\n",
    "            coordinates.\n",
    "        2. Retrieves and loads json file containing US counties and point \n",
    "            boundaries and adds this to a GeoDataFrame.\n",
    "        3. Produces a final GeoDataFrame (`joined_df`) using sjoin. That \n",
    "            contains only values from the `global_df` that contain coordinates\n",
    "            that exist within the bounds of one of the counties loaded into\n",
    "            `records_df` in step 2.\n",
    "        4. General cleanup.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Dataset retrieved from the most recent GDelta export.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        joined_df : Geopandas GeoDataFrame\n",
    "            Dataframe containing all values from the original df that have \n",
    "            coordinates within the United States.\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info('Prepping DataFrame with dimensions {}'.format(df.shape))\n",
    "        global_gdf = gpd.GeoDataFrame(\n",
    "            df, geometry=gpd.points_from_xy(df.ActionGeo_Long, df.ActionGeo_Lat),\n",
    "            crs=3857\n",
    "        )\n",
    "        global_gdf['geometry'].set_crs(epsg=3857, inplace=True)\n",
    "        logger.info('Confirming creation of new GeoDataFrame, shape {}'.format(\n",
    "            global_gdf.shape))\n",
    "        \n",
    "        with open(\"./geojson-counties-fips.json\", 'r') as j:\n",
    "            records = json.loads(j.read())\n",
    "            \n",
    "        records_gdf =  gpd.GeoDataFrame.from_features(records['features'], \n",
    "                      columns=['geometry'])        \n",
    "        records_gdf['geometry'].set_crs(epsg=3857, inplace=True)\n",
    "        logger.info('Created fips-records GeoDataFrame successfully')\n",
    "                                                              \n",
    "        joined_df = gpd.sjoin(left_df=global_gdf, right_df=records_gdf, how='inner')\n",
    "        #Map values using mappings\n",
    "        \n",
    "        joined_df.rename_geometry('ActionGeo_Coords', inplace=True)\n",
    "        joined_df.set_geometry('ActionGeo_Coords', inplace=True)\n",
    "        joined_df.drop('index_right', axis=1, inplace=True)\n",
    "        \n",
    "        logger.info('Stripped non-US datapoints from GeoDataFrame; shape {}.'.format(\n",
    "            joined_df.shape))\n",
    "        \n",
    "        return joined_df\n",
    "    \n",
    "    \n",
    "    def _process_latest(self, results: str) -> list:\n",
    "        \"\"\"Process results from `DataHandler.tar_url` to acquire the url of the\n",
    "        most recent export.\n",
    "        \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        results : str\n",
    "            list of ids and urls associated with the most recent batch of GDelta \n",
    "            exports.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            list of urls retrieved from GDelta referencing the most recent batch\n",
    "            of data.\n",
    "\n",
    "        \"\"\"\n",
    "        res_list = results.split('\\n')        \n",
    "        return [line.split(' ')[-1] for line in res_list if len(line) > 0]\n",
    "    \n",
    "    def _extract(self, zipped: str) -> pd.DataFrame:\n",
    "        \"\"\"Load the content of the zipped export csv into a Pandas DataFrame.\n",
    "        \n",
    "        We use the columns listed in the spec sheet, ignoring the rest of the \n",
    "        data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        zipped : str\n",
    "            Zipped file data as retrieved from the endpoint, converted from a \n",
    "            bytes string to a regular string for processing.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame\n",
    "            Dataframe containing the desired columns from the exported data.\n",
    "\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(zipped, sep='\\t', \n",
    "                 usecols=[0, 1, 26, 27, 28, 52, 53, 56, 57, 59, 60],\n",
    "                 names=[\n",
    "                     'GLOBALEVENTID',\n",
    "                     'SQLDATE',\n",
    "                     'EventCode',\n",
    "                     'EventBaseCode',\n",
    "                     'EventRootCode',\n",
    "                     'ActionGeo_FullName',\n",
    "                     'ActionGeo_CountryCode',\n",
    "                     'ActionGeo_Lat',\n",
    "                     'ActionGeo_Long',\n",
    "                     'DATEADDED',\n",
    "                     'SOURCEURL'\n",
    "                     ],\n",
    "                 converters={\n",
    "                     'EventCode': self._event_conv, \n",
    "                     'EventBaseCode': self._eventbase_conv,\n",
    "                     'EventRootCode': self._eventroot_conv,\n",
    "                     'ActionGeo_CountryCode': self._fips_iso2_conv},\n",
    "                 encoding='latin-1'\n",
    "                 )\n",
    "        return df\n",
    "    \n",
    "    def _event_conv(self, event_id: str) -> str:\n",
    "        \"\"\"Converts event codes to the mapped strings in `MAPPINGS[event_codes]`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        event_id : str\n",
    "            Event id to be processed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Corresponding plaintext version of the event code provided if one\n",
    "            exists, otherwise returns the original code.\n",
    "\n",
    "        \"\"\"\n",
    "        eid = str(event_id)\n",
    "        if eid in EVENT_CODES:\n",
    "            return EVENT_CODES[eid]\n",
    "        else:\n",
    "            logger.info('Event code not found in mapping: {}'.format(eid))\n",
    "            return eid\n",
    "    \n",
    "    def _eventbase_conv(self, event_id: int) -> str:\n",
    "        \"\"\"Converts event base codes to the mapped strings in `MAPPINGS[eventbase_codes]`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        event_id : str\n",
    "            Event id to be processed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Corresponding plaintext version of the event code provided if one\n",
    "            exists, otherwise returns the original code.\n",
    "\n",
    "        \"\"\"\n",
    "        eid = str(event_id)\n",
    "        if eid in EVENT_CODES:\n",
    "            return EVENT_BASE_CODES[eid]\n",
    "        else:\n",
    "            logger.info('Event code not found in mapping: {}'.format(eid))\n",
    "            return eid\n",
    "    \n",
    "    def _eventroot_conv(self, event_id: int) -> str:\n",
    "        \"\"\"Converts event root codes to the mapped strings in `MAPPINGS[event_codes]`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        event_id : str\n",
    "            Event id to be processed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Corresponding plaintext version of the event code provided if one\n",
    "            exists, otherwise returns the original code.\n",
    "\n",
    "        \"\"\"\n",
    "        eid = str(event_id)\n",
    "        if eid in EVENT_CODES:\n",
    "            return EVENT_ROOT_CODES[eid]\n",
    "        else:\n",
    "            logger.info('Event code not found in mapping: {}'.format(eid))\n",
    "            return eid\n",
    "    \n",
    "    def _fips_iso2_conv(self, fips_id: str) -> str:\n",
    "        \"\"\"Converts FIPS codes to ISO2 codes using `MAPPING[map_fips_to_iso2]`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        event_id : str\n",
    "            FIPS id to be processed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Corresponding plaintext version of the FIPS code provided if one\n",
    "            exists, otherwise returns the original code.\n",
    "\n",
    "        \"\"\"\n",
    "        fid = str(fips_id)\n",
    "        if fid in EVENT_CODES:\n",
    "            return MAP_FIPS_TO_ISO2[fid]\n",
    "        else:\n",
    "            logger.info('FIPS code not found in mapping: {}'.format(fid))\n",
    "            return fid\n",
    "\n",
    "class RunHandler(DataHandler):\n",
    "    \"\"\"Custom runner use with the `DataHandler` class.    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"RunHandler init function.\n",
    "        \n",
    "        Trigger parent classed initialization.\n",
    "\n",
    "        \"\"\"        \n",
    "        DataHandler.__init__(self)    \n",
    "    \n",
    "    def run(self) -> None:\n",
    "        results = str(self.reqs.get_content(DataHandler.tar_url), 'UTF-8')\n",
    "        urls_list = self._process_latest(results)\n",
    "        print(urls_list)\n",
    "        \n",
    "        '''\n",
    "        Instead of hardcoding (i.e. url = url_list[0]) use list comprehension\n",
    "        plus if statement to find the desired url. This protects against \n",
    "        failures if/when endpoint changes result in the returned list being in\n",
    "        an unexpected order.\n",
    "        '''\n",
    "        url = [url for url in urls_list if 'export' in url][0]\n",
    "        print(url)\n",
    "        \n",
    "        results = io.BytesIO(self.reqs.get_content(url))\n",
    "        \n",
    "        #Unpack zip\n",
    "        df_content = self._extract(results)\n",
    "        print(len(df_content.index))\n",
    "        \n",
    "        #Prep\n",
    "        gdf = self._data_prep(df_content)\n",
    "        \n",
    "        #Load into db\n",
    "        self.db.upload(gdf)\n",
    "        self.db.test()\n",
    "        self.db.close()\n",
    "    \n",
    "rh = RunHandler()\n",
    "rh.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48078c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
